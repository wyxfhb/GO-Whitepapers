# ðŸ“Š Benchmarking the Future: Comparing LabVIEW GPU Toolkits  
**CuLab, G2CPU, and the Graiphic Accelerator**

Welcome to the **Graiphic Benchmarking Whitepaper Repository**, where we share the **methods, results, and LabVIEW sources** used to compare the main GPU acceleration toolkits for **LabVIEW**.

This repository accompanies the official whitepaper:  
ðŸ‘‰ [**Benchmarking the Future: Comparing LabVIEW GPU Toolkits CuLab, G2CPU, and the Graiphic Accelerator (v1.1)**](./Benchmarking%20the%20Future%20Comparing%20LabVIEW%20GPU%20Toolkits%20CuLab%2C%20G2CPU%2C%20and%20the%20Graiphic%20Accelerator.1.1.pdf)

---

## ðŸ§© Overview

This benchmark measures and compares the **performance**, **integration**, and **determinism** of several LabVIEW GPU toolkits â€” all tested in the same LabVIEW environment.

### Toolkits Compared
- **Graiphic Accelerator Toolkit**
- **CuLab GPU Toolkit 4.1.2.80** (Ngene)
- **G2CPU GPU and CPU HPC Toolkit 1.6.0.15** (Natan Biesmans)
- **Native LabVIEW CPU execution**

The objective is to provide a **real-world comparison** and understand the trade-offs between speed, scalability, and ease of integration.

---

## âš™ï¸ Test Environment

| Component | Specification |
|------------|---------------|
| **OS** | Windows 11 |
| **CPU** | IntelÂ® Coreâ„¢ i9-10850K @ 3.60 GHz |
| **GPU** | NVIDIA GeForce RTX 3060 |
| **LabVIEW** | 2025 Q3 |
| **CUDA** | 12.8 |
| **TensorRT** | 10.13.3.9 |
| **DirectML** | 1.15.4.0 |
| **Date** | November 6, 2025 |

This setup represents a balanced workstation configuration for reproducible LabVIEW GPU benchmarks.

---

## ðŸ“š Benchmarks Included

1. **GEMM Processing**  
   Matrix multiplication followed by arithmetic post-processing.  
2. **Arithmetic Operations**  
   Iterative Add / Neg / Mul / Div loops for element-wise operations.  
3. **Complex Number Computation**  
   Handling of real + imaginary tensors using ONNX custom nodes.  
4. **Signal Processing Application**  
   FFT + arithmetic operations on **real NI-like signal data (~32 k samples)**.  
   âž¤ This test was designed to reflect realistic, small-scale sensor workloads â€” not synthetic stress tests.

---

## ðŸ§  Key Findings

- **Graiphic Accelerator (TensorRT)** achieves the **highest performance**, up to:  
  - âš¡ **5Ã— faster than CuLab**  
  - âš¡ **40Ã— faster than G2CPU**  

- **Compiled-graph execution (ONNX Runtime)** significantly reduces overhead versus **per-node DLL execution**.

- **Complex-number support** is functional via custom ONNX nodes â€” an open research topic for future native integration.

- For **small data blocks**, CPU execution remains competitive; GPU benefits grow with workload size.  
  âž¤ The goal is *understanding performance behavior*, not claiming absolute superiority.

---

## ðŸ§ª Source Files

All LabVIEW VIs used to generate the benchmark results are available in the  
[`/Source`](./Source) directory.

| Benchmark | Folder | Description |
|------------|---------|-------------|
| **GEMM** | [Source/GEMM](./Source/GEMM) | Matrix-multiplication tests |
| **Arithmetic** | [Source/Not Complex](./Source/Not%20Complex) | Scalar & vector operations |
| **Complex** | [Source/Complex](./Source/Complex) | Custom complex-number computation |
| **Signal Processing** | [Source/Signal Processing Without Indicator And Warmup](./Source/Signal%20Processing%20Without%20Indicator%20And%20Warmup) | FFT-based signal test |

> **Note:** One additional large file is required for full signal-processing reproduction:   
> - `TEMP.BIN` (2 GB, test data file)  
>   ðŸ”— [Download here](http://download2.graiphic.io/_Bench/TEMP.BIN)

---

## ðŸ”¬ Replication & Discussion

This benchmark was built for **transparency, reproducibility, and community collaboration**.  
All materials (VIs, datasets, configurations) are public â€” anyone can rerun or extend the tests.

We strongly encourage:
- ðŸ” Independent replication using different GPUs, CPUs, or LabVIEW versions  
- ðŸ“ˆ Comparative submissions via pull requests or GitHub issues  
- ðŸ§© Proposals for new test cases (e.g., deep learning workloads, larger FFTs, etc.)  
- ðŸ§  Constructive discussion on methodology and interpretation  

> **Benchmarking is not competition â€” itâ€™s collaboration.**  
> Our goal is to build shared understanding, not to declare winners.

ðŸ“¢ **Discussion board:** Use [GitHub Issues](https://github.com/Graiphic/whitepapers/issues) to post replication results, observations, or suggestions.  
ðŸ“ **Repository:** [https://github.com/Graiphic/whitepapers](https://github.com/Graiphic/whitepapers)

---

## ðŸš€ About Graiphic

**Graiphic** develops the first ecosystem unifying **AI + Logic + Hardware + Energy** inside a single **ONNX graph**.  
Our technology, **GO HW (Graph Orchestration Hardware)**, enables a universal, hardware-agnostic execution layer bridging **ONNX Runtime**, **MLIR**, and **LabVIEW**.

### ðŸ“¬ Get in Touch
- ðŸ’¡ Funding & Partnerships â†’ [funding@graiphic.io](mailto:funding@graiphic.io)  
- ðŸŒ Website â†’ [www.graiphic.io](https://www.graiphic.io)

---

## ðŸ—“ï¸ Versioning

| Version | Date | Author | Description |
|----------|------|--------|-------------|
| **1.0** | 2025-10-15 | Youssef Menjour (Graiphic) | First release of benchmarking whitepaper and LabVIEW sources |
| **1.1** | 2025-11-07 | Youssef Menjour (Graiphic) | Added DirectML execution provider |

---

## ðŸ§® Towards a Community Standard: LabVIEW Open Benchmark Suite (LOBS)

Following this first Graiphic benchmark, weâ€™ve launched the  
**[LabVIEW Open Benchmark Suite (LOBS)](./LabVIEW%20Open%20Benchmark%20Suite)** â€”  
a collaborative initiative inspired by **SPEC** and **MLPerf**, aimed at building  
a **shared, reproducible standard** for LabVIEW performance evaluation.

LOBS extends this initial study by providing:
- ðŸ§  **Open-source, vendor-neutral test cases** (FFT, GEMM, AI, etc.)  
- ðŸ” **Reproducible pipelines** anyone can execute and extend  
- ðŸ“Š **Transparent comparison criteria** (time, precision, energy, determinism)  

If you want to replicate or contribute to the next wave of open benchmarks,  
ðŸ‘‰ start here: [**LabVIEW Open Benchmark Suite â†’**](./LabVIEW%20Open%20Benchmark%20Suite)

> This whitepaper is the **Reference 0** of the suite â€”  
> the foundation on which all future community benchmarks will build.

---


